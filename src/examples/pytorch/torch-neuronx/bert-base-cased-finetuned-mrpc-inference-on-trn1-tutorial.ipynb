{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11b2ce1",
   "metadata": {},
   "source": [
    "# Compiling and Deploying HuggingFace Pretrained BERT on Trn1 or Inf2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a44364",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial we will compile and deploy a HuggingFace ðŸ¤— Transformers BERT model for accelerated inference on Neuron.\n",
    "\n",
    "This tutorial will use the [bert-base-cased-finetuned-mrpc](https://huggingface.co/bert-base-cased-finetuned-mrpc) model. This model has 12 layers, 768 hidden dimensions, 12 attention heads, and 110M total parameters. The final layer is a binary classification head that has been trained on the Microsoft Research Paraphrase Corpus (`mrpc`). The input to the model is two sentences and the output of the model is whether or not those sentences are a paraphrase of each other.\n",
    "\n",
    "This tutorial has the following main sections:\n",
    "\n",
    "1. Install dependencies\n",
    "1. Compile the BERT model\n",
    "1. Run inference on Neuron and compare results to CPU\n",
    "1. Benchmark the model using multicore inference\n",
    "1. Finding the optimal batch size\n",
    "\n",
    "This Jupyter notebook should be run on a Trn1 instance (`trn1.2xlarge` or larger.) or Inf2 instance (`inf2.xlarge` or larger.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceecb92",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "The code in this tutorial is written for Jupyter Notebooks. To use Jupyter Notebook on the Neuron instance, you\n",
    "can use this [guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html).\n",
    "\n",
    "This tutorial requires the following pip packages:\n",
    "\n",
    "- `torch-neuronx`\n",
    "- `neuronx-cc`\n",
    "- `transformers`\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the Trn1/Inf2 setup guide. The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66392b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (4.26.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers) (1.20.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers) (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82533d8e",
   "metadata": {},
   "source": [
    "## Compile the model into an AWS Neuron optimized TorchScript\n",
    "\n",
    "In the following section, we load the BERT model and tokenizer, get a sample input, run inference on CPU, compile the model for Neuron using `torch_neuronx.trace()`, and save the optimized model as `TorchScript`.\n",
    "\n",
    "`torch_neuronx.trace()` expects a tensor or tuple of tensor inputs to use for tracing, so we unpack the tokenizer output using the `encode` function. \n",
    "\n",
    "The result of the trace stage will be a static executable where the operations to be run upon inference are determined during compilation. This means that when inferring, the resulting Neuron model must be executed with tensors that are the exact same shape as those provided at compilation time. If a model is given a tensor at inference time whose shape does not match the tensor given at compilation time, an error will occur.\n",
    "\n",
    "For language models, the shape of the tokenizer tensors can vary based on the length of input sentence. We can satisfy the Neuron restriction of using a fixed shape input by padding all varying input tensors to a specified length. In a deployment scenario, the padding size should be chosen based on the maximum token length that is expected to occur for the application.\n",
    "\n",
    "In the following section we will assume that we will receive a maximum of 128 tokens at inference time. We will pad our example inputs by using `padding='max_length'` and to avoid potential errors caused by creating a tensor that is larger than `max_length=128`, we will always tokenize using `truncation=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9aac5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2023 09:18:09 AM WARNING 44057 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/05/2023 09:18:29 AM WARNING 44057 [WalrusDriver]: 100% PSUM demand before spilling\n",
      "03/05/2023 09:18:29 AM WARNING 44057 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/05/2023 09:18:29 AM WARNING 44057 [WalrusDriver]: 100% PSUM utilization after allocation\n",
      "03/05/2023 09:18:29 AM WARNING 44057 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/05/2023 09:18:29 AM WARNING 44057 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/05/2023 09:18:29 AM WARNING 44057 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/05/2023 09:18:29 AM WARNING 44057 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/05/2023 09:18:29 AM WARNING 44057 [WalrusDriver]: DRAM allocation successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import transformers\n",
    "\n",
    "\n",
    "def encode(tokenizer, *inputs, max_length=128, batch_size=1):\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        *inputs,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return (\n",
    "        torch.repeat_interleave(tokens['input_ids'], batch_size, 0),\n",
    "        torch.repeat_interleave(tokens['attention_mask'], batch_size, 0),\n",
    "        torch.repeat_interleave(tokens['token_type_ids'], batch_size, 0),\n",
    "    )\n",
    "\n",
    "\n",
    "# Create the tokenizer and model\n",
    "name = \"bert-base-cased-finetuned-mrpc\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name, torchscript=True)\n",
    "\n",
    "# Set up some example inputs\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "paraphrase = encode(tokenizer, sequence_0, sequence_2)\n",
    "not_paraphrase = encode(tokenizer, sequence_0, sequence_1)\n",
    "\n",
    "# Run the original PyTorch BERT model on CPU\n",
    "cpu_paraphrase_logits = model(*paraphrase)[0]\n",
    "cpu_not_paraphrase_logits = model(*not_paraphrase)[0]\n",
    "\n",
    "# Compile the model for Neuron\n",
    "model_neuron = torch_neuronx.trace(model, paraphrase)\n",
    "\n",
    "# Save the TorchScript for inference deployment\n",
    "filename = 'model.pt'\n",
    "torch.jit.save(model_neuron, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9605d",
   "metadata": {},
   "source": [
    "## Run inference and compare results\n",
    "\n",
    "In this section we load the compiled model, run inference on Neuron, and compare the CPU and Neuron outputs.\n",
    "\n",
    "NOTE: Although this tutorial section uses one NeuronCore (and the next section uses two NeuronCores), by default each Jupyter notebook Python process will attempt to take ownership of all NeuronCores visible on the instance. For multi-process applications where each process should only use a subset of the NeuronCores on the instance you can use NEURON_RT_NUM_CORES=N or NEURON_RT_VISIBLE_CORES=< list of NeuronCore IDs > when starting the Jupyter notebook as described in [NeuronCore Allocation and Model Placement for Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/inference/core-placement.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d509aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU paraphrase logits:         [[-0.34945598  1.9003887 ]]\n",
      "Neuron paraphrase logits:     [[-0.34927163  1.8980128 ]]\n",
      "CPU not-paraphrase logits:     [[ 0.5386365 -2.2197142]]\n",
      "Neuron not-paraphrase logits:  [[ 0.53779405 -2.2180367 ]]\n"
     ]
    }
   ],
   "source": [
    "# Load the TorchScript compiled model\n",
    "model_neuron = torch.jit.load(filename)\n",
    "\n",
    "# Verify the TorchScript works on both example inputs\n",
    "neuron_paraphrase_logits = model_neuron(*paraphrase)[0]\n",
    "neuron_not_paraphrase_logits = model_neuron(*not_paraphrase)[0]\n",
    "\n",
    "# Compare the results\n",
    "print('CPU paraphrase logits:        ', cpu_paraphrase_logits.detach().numpy())\n",
    "print('Neuron paraphrase logits:    ', neuron_paraphrase_logits.detach().numpy())\n",
    "print('CPU not-paraphrase logits:    ', cpu_not_paraphrase_logits.detach().numpy())\n",
    "print('Neuron not-paraphrase logits: ', neuron_not_paraphrase_logits.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4553cc9",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "In this section we benchmark the performance of the BERT model on Neuron. By default, models compiled with `torch_neuronx` will always execute on a *single* NeuronCore. When loading *multiple* models, the default behavior of the Neuron runtime is to evenly distribute models across all available NeuronCores. The runtime places models on the NeuronCore that has the fewest models loaded to it first. In the following section, we will `torch.jit.load` multiple instances of the model which should each be loaded onto their own NeuronCore. It is not useful to load more copies of a model than the number of NeuronCores on the instance since an individual NeuronCore can only execute one model at a time.\n",
    "\n",
    "To ensure that we are maximizing hardware utilization, we must run inferences using multiple threads in parallel. It is nearly always recommended to use some form of threading/multiprocessing and some form of model replication since even the smallest Neuron EC2 instance has 2 NeuronCores available. Applications with no form of threading are only capable of `1 / num_neuron_cores` hardware utilization which becomes especially problematic on large instances.\n",
    "\n",
    "One way to view the hardware utilization is by executing the `neuron-top` application in the terminal while the benchmark is executing. If the monitor shows >90% utilization on all NeuronCores, this is a good indication that the hardware is being utilized effectively.\n",
    "\n",
    "In this example we load two models, which utilizes all NeuronCores (2) on a `trn1.2xlarge` or `inf2.xlarge` instance. Additional models can be loaded and run in parallel on larger Trn1 or Inf2 instance sizes to increase throughput.\n",
    "\n",
    "We define a benchmarking function that loads two optimized BERT models onto two separate NeuronCores, runs multithreaded inference, and calculates the corresponding latency and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9e14b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename:    model.pt\n",
      "Batch Size:  1\n",
      "Batches:     2000\n",
      "Inferences:  2000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    1.148\n",
      "Throughput:  1741.693\n",
      "Latency P50: 1.150\n",
      "Latency P95: 1.181\n",
      "Latency P99: 1.254\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def benchmark(filename, example, n_models=2, n_threads=2, batches_per_thread=1000):\n",
    "    \"\"\"\n",
    "    Record performance statistics for a serialized model and its input example.\n",
    "\n",
    "    Arguments:\n",
    "        filename: The serialized torchscript model to load for benchmarking.\n",
    "        example: An example model input.\n",
    "        n_models: The number of models to load.\n",
    "        n_threads: The number of simultaneous threads to execute inferences on.\n",
    "        batches_per_thread: The number of example batches to run per thread.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of performance statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load models\n",
    "    models = [torch.jit.load(filename) for _ in range(n_models)]\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(8):\n",
    "        for model in models:\n",
    "            model(*example)\n",
    "\n",
    "    latencies = []\n",
    "\n",
    "    # Thread task\n",
    "    def task(model):\n",
    "        for _ in range(batches_per_thread):\n",
    "            start = time.time()\n",
    "            model(*example)\n",
    "            finish = time.time()\n",
    "            latencies.append((finish - start) * 1000)\n",
    "\n",
    "    # Submit tasks\n",
    "    begin = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_threads) as pool:\n",
    "        for i in range(n_threads):\n",
    "            pool.submit(task, models[i % len(models)])\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute metrics\n",
    "    boundaries = [50, 95, 99]\n",
    "    percentiles = {}\n",
    "\n",
    "    for boundary in boundaries:\n",
    "        name = f'latency_p{boundary}'\n",
    "        percentiles[name] = np.percentile(latencies, boundary)\n",
    "    duration = end - begin\n",
    "    batch_size = 0\n",
    "    for tensor in example:\n",
    "        if batch_size == 0:\n",
    "            batch_size = tensor.shape[0]\n",
    "    inferences = len(latencies) * batch_size\n",
    "    throughput = inferences / duration\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'filename': str(filename),\n",
    "        'batch_size': batch_size,\n",
    "        'batches': len(latencies),\n",
    "        'inferences': inferences,\n",
    "        'threads': n_threads,\n",
    "        'models': n_models,\n",
    "        'duration': duration,\n",
    "        'throughput': throughput,\n",
    "        **percentiles,\n",
    "    }\n",
    "\n",
    "    display(metrics)\n",
    "\n",
    "\n",
    "def display(metrics):\n",
    "    \"\"\"\n",
    "    Display the metrics produced by `benchmark` function.\n",
    "\n",
    "    Args:\n",
    "        metrics: A dictionary of performance statistics.\n",
    "    \"\"\"\n",
    "    pad = max(map(len, metrics)) + 1\n",
    "    for key, value in metrics.items():\n",
    "\n",
    "        parts = key.split('_')\n",
    "        parts = list(map(str.title, parts))\n",
    "        title = ' '.join(parts) + \":\"\n",
    "\n",
    "        if isinstance(value, float):\n",
    "            value = f'{value:0.3f}'\n",
    "\n",
    "        print(f'{title :<{pad}} {value}')\n",
    "\n",
    "\n",
    "# Benchmark BERT on Neuron\n",
    "benchmark(filename, paraphrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc374b12",
   "metadata": {},
   "source": [
    "## Finding the optimal batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113acb55",
   "metadata": {},
   "source": [
    "Batch size has a direct impact on model performance. The NeuronCore architecture is optimized to maximize throughput with relatively small batch sizes. This means that a Neuron compiled model can outperform a GPU model, even if running single digit batch sizes.\n",
    "\n",
    "As a general best practice, we recommend optimizing your modelâ€™s throughput by compiling the model with a small batch size and gradually increasing it to find the peak throughput on Neuron. As a general best practice, we recommend optimizing your modelâ€™s throughput by compiling the model with a small batch size and gradually increasing it to find the peak throughput on Neuron. To minimize latency, using `batch size = 1` will nearly always be optimal. This batch size configuration is typically used for on-demand inference applications. To maximize throughput, *usually* `1 < batch_size < 10` is optimal. A configuration which uses a larger batch size is generally ideal for batched on-demand inference or offline batch processing.\n",
    "\n",
    "In the following section, we compile BERT for multiple batch size inputs. We then run inference on each batch size and benchmark the performance. Notice that latency increases consistently as the batch size increases. Throughput increases as well, up until a certain point where the input size becomes too large to be efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be26aafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2023 09:18:46 AM WARNING 45095 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/05/2023 09:19:06 AM WARNING 45095 [WalrusDriver]: 100% PSUM demand before spilling\n",
      "03/05/2023 09:19:06 AM WARNING 45095 [WalrusDriver]: spilling from PSUM cost about 0 cycles\n",
      "03/05/2023 09:19:06 AM WARNING 45095 [WalrusDriver]: 100% PSUM utilization after allocation\n",
      "03/05/2023 09:19:06 AM WARNING 45095 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/05/2023 09:19:06 AM WARNING 45095 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/05/2023 09:19:06 AM WARNING 45095 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/05/2023 09:19:06 AM WARNING 45095 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/05/2023 09:19:06 AM WARNING 45095 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2023 09:19:16 AM WARNING 46164 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/05/2023 09:19:42 AM WARNING 46164 [WalrusDriver]: 200% PSUM demand before spilling\n",
      "03/05/2023 09:19:42 AM WARNING 46164 [WalrusDriver]: spilling from PSUM cost about 16704 cycles\n",
      "03/05/2023 09:19:42 AM WARNING 46164 [WalrusDriver]: 100% PSUM utilization after allocation\n",
      "03/05/2023 09:19:42 AM WARNING 46164 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/05/2023 09:19:42 AM WARNING 46164 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/05/2023 09:19:42 AM WARNING 46164 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/05/2023 09:19:42 AM WARNING 46164 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/05/2023 09:19:42 AM WARNING 46164 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2023 09:19:52 AM WARNING 47178 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/05/2023 09:20:18 AM WARNING 47178 [WalrusDriver]: 300% PSUM demand before spilling\n",
      "03/05/2023 09:20:18 AM WARNING 47178 [WalrusDriver]: spilling from PSUM cost about 3528 cycles\n",
      "03/05/2023 09:20:18 AM WARNING 47178 [WalrusDriver]: 100% PSUM utilization after allocation\n",
      "03/05/2023 09:20:18 AM WARNING 47178 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/05/2023 09:20:18 AM WARNING 47178 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/05/2023 09:20:18 AM WARNING 47178 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/05/2023 09:20:18 AM WARNING 47178 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/05/2023 09:20:18 AM WARNING 47178 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2023 09:20:29 AM WARNING 48205 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/05/2023 09:20:54 AM WARNING 48205 [WalrusDriver]: 600% PSUM demand before spilling\n",
      "03/05/2023 09:20:54 AM WARNING 48205 [WalrusDriver]: spilling from PSUM cost about 67752 cycles\n",
      "03/05/2023 09:20:54 AM WARNING 48205 [WalrusDriver]: 100% PSUM utilization after allocation\n",
      "03/05/2023 09:20:54 AM WARNING 48205 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/05/2023 09:20:54 AM WARNING 48205 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/05/2023 09:20:54 AM WARNING 48205 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/05/2023 09:20:54 AM WARNING 48205 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/05/2023 09:20:54 AM WARNING 48205 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2023 09:21:05 AM WARNING 49233 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/05/2023 09:21:43 AM WARNING 49233 [WalrusDriver]: 375% PSUM demand before spilling\n",
      "03/05/2023 09:21:43 AM WARNING 49233 [WalrusDriver]: spilling from PSUM cost about 68976 cycles\n",
      "03/05/2023 09:21:43 AM WARNING 49233 [WalrusDriver]: 100% PSUM utilization after allocation\n",
      "03/05/2023 09:21:43 AM WARNING 49233 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/05/2023 09:21:43 AM WARNING 49233 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/05/2023 09:21:43 AM WARNING 49233 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/05/2023 09:21:43 AM WARNING 49233 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/05/2023 09:21:43 AM WARNING 49233 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2023 09:21:54 AM WARNING 50261 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/05/2023 09:22:21 AM WARNING 50261 [WalrusDriver]: 450% PSUM demand before spilling\n",
      "03/05/2023 09:22:21 AM WARNING 50261 [WalrusDriver]: spilling from PSUM cost about 131571 cycles\n",
      "03/05/2023 09:22:21 AM WARNING 50261 [WalrusDriver]: 100% PSUM utilization after allocation\n",
      "03/05/2023 09:22:21 AM WARNING 50261 [WalrusDriver]: spilling from SB cost about 0 cycles\n",
      "03/05/2023 09:22:21 AM WARNING 50261 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/05/2023 09:22:21 AM WARNING 50261 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/05/2023 09:22:21 AM WARNING 50261 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/05/2023 09:22:21 AM WARNING 50261 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2023 09:22:33 AM WARNING 51320 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/05/2023 09:23:10 AM WARNING 51320 [WalrusDriver]: 525% PSUM demand before spilling\n",
      "03/05/2023 09:23:11 AM WARNING 51320 [WalrusDriver]: spilling from PSUM cost about 113619 cycles\n",
      "03/05/2023 09:23:11 AM WARNING 51320 [WalrusDriver]: 100% PSUM utilization after allocation\n",
      "03/05/2023 09:23:11 AM WARNING 51320 [WalrusDriver]: spilling from SB cost about 174864 cycles\n",
      "03/05/2023 09:23:11 AM WARNING 51320 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/05/2023 09:23:11 AM WARNING 51320 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/05/2023 09:23:11 AM WARNING 51320 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/05/2023 09:23:11 AM WARNING 51320 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2023 09:23:23 AM WARNING 52348 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/05/2023 09:23:50 AM WARNING 52348 [WalrusDriver]: 600% PSUM demand before spilling\n",
      "03/05/2023 09:23:50 AM WARNING 52348 [WalrusDriver]: spilling from PSUM cost about 138120 cycles\n",
      "03/05/2023 09:23:50 AM WARNING 52348 [WalrusDriver]: 100% PSUM utilization after allocation\n",
      "03/05/2023 09:23:50 AM WARNING 52348 [WalrusDriver]: spilling from SB cost about 188800 cycles\n",
      "03/05/2023 09:23:50 AM WARNING 52348 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/05/2023 09:23:50 AM WARNING 52348 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/05/2023 09:23:50 AM WARNING 52348 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/05/2023 09:23:50 AM WARNING 52348 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2023 09:24:02 AM WARNING 53364 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/05/2023 09:24:30 AM WARNING 53364 [WalrusDriver]: 225% PSUM demand before spilling\n",
      "03/05/2023 09:24:31 AM WARNING 53364 [WalrusDriver]: spilling from PSUM cost about 24630 cycles\n",
      "03/05/2023 09:24:31 AM WARNING 53364 [WalrusDriver]: 100% PSUM utilization after allocation\n",
      "03/05/2023 09:24:31 AM WARNING 53364 [WalrusDriver]: spilling from SB cost about 61194 cycles\n",
      "03/05/2023 09:24:31 AM WARNING 53364 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/05/2023 09:24:31 AM WARNING 53364 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/05/2023 09:24:31 AM WARNING 53364 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/05/2023 09:24:31 AM WARNING 53364 [WalrusDriver]: DRAM allocation successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2023 09:24:44 AM WARNING 54378 [py.warnings]: /home/ubuntu/aws_neuron_venv_pytorch/bin/neuronx-cc:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sys.exit(main())\n",
      "\n",
      "03/05/2023 09:25:20 AM WARNING 54378 [WalrusDriver]: 300% PSUM demand before spilling\n",
      "03/05/2023 09:25:20 AM WARNING 54378 [WalrusDriver]: spilling from PSUM cost about 154629 cycles\n",
      "03/05/2023 09:25:20 AM WARNING 54378 [WalrusDriver]: 100% PSUM utilization after allocation\n",
      "03/05/2023 09:25:21 AM WARNING 54378 [WalrusDriver]: spilling from SB cost about 1.54958e+06 cycles\n",
      "03/05/2023 09:25:21 AM WARNING 54378 [WalrusDriver]: 0 bytes/partition (0%) successfully pinned\n",
      "03/05/2023 09:25:21 AM WARNING 54378 [WalrusDriver]: pinning saved approximately 0 cycles\n",
      "03/05/2023 09:25:21 AM WARNING 54378 [WalrusDriver]: 0% SB utilization after allocation\n",
      "03/05/2023 09:25:21 AM WARNING 54378 [WalrusDriver]: DRAM allocation successful\n"
     ]
    }
   ],
   "source": [
    "# Compile BERT for different batch sizes\n",
    "for batch_size in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(name, torchscript=True)\n",
    "    example = encode(tokenizer, sequence_0, sequence_2, batch_size=batch_size)\n",
    "    model_neuron = torch_neuronx.trace(model, example)\n",
    "    filename = f'model_batch_size_{batch_size}.pt'\n",
    "    torch.jit.save(model_neuron, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f0f6ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_1.pt\n",
      "Batch Size:  1\n",
      "Batches:     2000\n",
      "Inferences:  2000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    1.144\n",
      "Throughput:  1747.653\n",
      "Latency P50: 1.140\n",
      "Latency P95: 1.164\n",
      "Latency P99: 1.208\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_2.pt\n",
      "Batch Size:  2\n",
      "Batches:     2000\n",
      "Inferences:  4000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    1.714\n",
      "Throughput:  2334.359\n",
      "Latency P50: 1.713\n",
      "Latency P95: 1.737\n",
      "Latency P99: 1.757\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_3.pt\n",
      "Batch Size:  3\n",
      "Batches:     2000\n",
      "Inferences:  6000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    2.389\n",
      "Throughput:  2511.216\n",
      "Latency P50: 2.382\n",
      "Latency P95: 2.439\n",
      "Latency P99: 2.504\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_4.pt\n",
      "Batch Size:  4\n",
      "Batches:     2000\n",
      "Inferences:  8000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    2.892\n",
      "Throughput:  2766.519\n",
      "Latency P50: 2.888\n",
      "Latency P95: 2.919\n",
      "Latency P99: 2.949\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_5.pt\n",
      "Batch Size:  5\n",
      "Batches:     2000\n",
      "Inferences:  10000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    3.786\n",
      "Throughput:  2641.547\n",
      "Latency P50: 3.768\n",
      "Latency P95: 3.834\n",
      "Latency P99: 3.902\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_6.pt\n",
      "Batch Size:  6\n",
      "Batches:     2000\n",
      "Inferences:  12000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    4.323\n",
      "Throughput:  2775.870\n",
      "Latency P50: 4.302\n",
      "Latency P95: 4.591\n",
      "Latency P99: 4.620\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_7.pt\n",
      "Batch Size:  7\n",
      "Batches:     2000\n",
      "Inferences:  14000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    5.104\n",
      "Throughput:  2743.200\n",
      "Latency P50: 5.081\n",
      "Latency P95: 5.188\n",
      "Latency P99: 5.239\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_8.pt\n",
      "Batch Size:  8\n",
      "Batches:     2000\n",
      "Inferences:  16000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    5.766\n",
      "Throughput:  2774.707\n",
      "Latency P50: 5.754\n",
      "Latency P95: 5.840\n",
      "Latency P99: 5.884\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_9.pt\n",
      "Batch Size:  9\n",
      "Batches:     2000\n",
      "Inferences:  18000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    6.315\n",
      "Throughput:  2850.447\n",
      "Latency P50: 6.250\n",
      "Latency P95: 6.585\n",
      "Latency P99: 6.639\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_10.pt\n",
      "Batch Size:  10\n",
      "Batches:     2000\n",
      "Inferences:  20000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    7.523\n",
      "Throughput:  2658.568\n",
      "Latency P50: 7.391\n",
      "Latency P95: 8.107\n",
      "Latency P99: 8.127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Benchmark BERT for different batch sizes\n",
    "for batch_size in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    print('-'*50)\n",
    "    example = encode(tokenizer, sequence_0, sequence_2, batch_size=batch_size)\n",
    "    filename = f'model_batch_size_{batch_size}.pt'\n",
    "    benchmark(filename, example)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-neuronx)",
   "language": "python",
   "name": "aws_neuron_venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
